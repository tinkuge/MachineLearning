{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"25 autograd.ipynb","provenance":[{"file_id":"https://github.com/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_preliminaries/autograd.ipynb","timestamp":1595899821570}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"HcffTtStAFvc","colab_type":"text"},"source":["# Automatic Differentiation\n",":label:`sec_autograd`\n","\n","As we have explained in :numref:`sec_calculus`,\n","differentiation is a crucial step in nearly all deep learning optimization algorithms.\n","While the calculations for taking these derivatives are straightforward,\n","requiring only some basic calculus,\n","for complex models, working out the updates by hand\n","can be a pain (and often error-prone).\n","\n","Deep learning frameworks expedite this work\n","by automatically calculating derivatives, i.e., *automatic differentiation*.\n","In practice,\n","based on our designed model\n","the system builds a *computational graph*,\n","tracking which data combined through\n","which operations to produce the output.\n","Automatic differentiation enables the system to subsequently backpropagate gradients.\n","Here, *backpropagate* simply means to trace through the computational graph,\n","filling in the partial derivatives with respect to each parameter.\n"]},{"cell_type":"markdown","metadata":{"id":"o4ucUI7flFHw","colab_type":"text"},"source":["[Explanation of AutoDiff](https://medium.com/@marksaroufim/automatic-differentiation-step-by-step-24240f97a6e6)\n","\n","[Gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient)"]},{"cell_type":"markdown","metadata":{"id":"7A5ZzoSsbR1H","colab_type":"text"},"source":["Remember that gradient is a vector of partial differentiation with respect to inputs"]},{"cell_type":"code","metadata":{"origin_pos":3,"tab":["tensorflow"],"id":"MX2pBUH9AFvd","colab_type":"code","colab":{}},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57Rk1nk-hcIf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596017578545,"user_tz":420,"elapsed":422,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"a45cace9-2183-40f9-a960-ad06728557cd"},"source":["x = tf.constant(5.0)\n","with tf.GradientTape(persistent=True) as tape:\n","    tape.watch(x)\n","    y = x**3\n","\n","print(tape.gradient(y, x).numpy()) # -> 75.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["75.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AbypJtmwiZcR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596017579575,"user_tz":420,"elapsed":389,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"eda2ef28-856f-430f-fcf6-e3975827d985"},"source":["print(tape.gradient(y, x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(75.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"JGNaf60jAFvi","colab_type":"text"},"source":["## A Simple Example\n","\n","As a toy example, say that we are interested\n","in differentiating the function\n","$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n","with respect to the column vector $\\mathbf{x}$.\n","To start, let us create the variable `x` and assign it an initial value.\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"mtMUQYg2AFvi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596017647227,"user_tz":420,"elapsed":397,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"1fcca69b-cc06-44e6-85b8-a6073d652e7e"},"source":["x = tf.range(4, dtype=tf.float32)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"F8LXJaRxAFvm","colab_type":"text"},"source":["Before we even calculate the gradient\n","of $y$ with respect to $\\mathbf{x}$,\n","we will need a place to store it.\n","It is important that we do not allocate new memory\n","every time we take a derivative with respect to a parameter\n","because we will often update the same parameters\n","thousands or millions of times\n","and could quickly run out of memory.\n","Note that a gradient of a scalar-valued function\n","with respect to a vector $\\mathbf{x}$\n","is itself vector-valued and has the same shape as $\\mathbf{x}$.\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"Mm-3RErYAFvn","colab_type":"code","colab":{}},"source":["x = tf.Variable(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U-P2yy6BGAz1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596017796511,"user_tz":420,"elapsed":393,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"8342ea3d-a220-4752-dca3-0600fa4e9564"},"source":["tf.tensordot(x,x, axes = 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=14.0>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"pSjm-JQJAFvp","colab_type":"text"},"source":["Now let us calculate $y$.\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["tensorflow"],"id":"WXrwYUV5AFvp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596019878091,"user_tz":420,"elapsed":425,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"516b1723-a004-4d20-b041-9078a31ae705"},"source":["# Record all computations onto a tape\n","with tf.GradientTape() as t:\n","    y = 2 * tf.tensordot(x, x, axes=1)\n","y #y amounts to y = 2*(X*X-Transpose)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"origin_pos":16,"id":"yhu3nNCzAFvs","colab_type":"text"},"source":["Since `x` is a vector of length 4,\n","an inner product of `x` and `x` is performed,\n","yielding the scalar output that we assign to `y`.\n","Next, we can automatically calculate the gradient of `y`\n","with respect to each component of `x`\n","by calling the function for backpropagation and printing the gradient.\n"]},{"cell_type":"code","metadata":{"origin_pos":19,"tab":["tensorflow"],"id":"7mGsQq4DAFvt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596019890458,"user_tz":420,"elapsed":402,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"243fdbfb-406f-40f4-b6b7-5ac50e67df94"},"source":["x_grad = t.gradient(y, x) #basically differentiating y wrt x\n","print(x_grad)\n","print(x_grad.numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([ 0.  4.  8. 12.], shape=(4,), dtype=float32)\n","[ 0.  4.  8. 12.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":20,"id":"E0ZJ834BAFvw","colab_type":"text"},"source":["The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n","with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.\n","Let us quickly verify that our desired gradient was calculated correctly.\n"]},{"cell_type":"code","metadata":{"origin_pos":23,"tab":["tensorflow"],"id":"7M-PhGu2AFvw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596020423666,"user_tz":420,"elapsed":6846,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"068c1bcb-0875-422b-b5b5-ce42b5b5cb9b"},"source":["x_grad == 4 * x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"origin_pos":24,"id":"uHr0rtD8AFv0","colab_type":"text"},"source":["Now let us calculate another function of `x`.\n"]},{"cell_type":"markdown","metadata":{"id":"EdKSf5GDeFhm","colab_type":"text"},"source":["`y` is a scalar valued function meaning it spits out a scalar value despite being made up of multiple different inputs. One example is a function that takes latitude and longitude as inputs and spits out the current temperature as the output."]},{"cell_type":"code","metadata":{"origin_pos":27,"tab":["tensorflow"],"id":"ivFN3kLIAFv0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596025037247,"user_tz":420,"elapsed":506,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"a955262c-9afb-4bad-b325-d0164f83a2fb"},"source":["with tf.GradientTape() as t:\n","    y = tf.reduce_sum(x) #this is  a scalar valued function \n","    #meaning it returns a singular scalar value despite taking multiple different \n","    #inputs (values inside tensor x)\n","t.gradient(y, x)  # Overwritten by the newly calculated gradient"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"ru86ug5XZ4ry","colab_type":"text"},"source":["Result when a scalar valued function is partially differentiated over a vector, is still a vector. In other words, gradient of a scalar valued function over a vector is still a vector. The result has the same shape as the vector.\n","\n","See: [Gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) for proof"]},{"cell_type":"markdown","metadata":{"origin_pos":28,"id":"GkjlUBEWAFv3","colab_type":"text"},"source":["## Backward for Non-Scalar Variables\n","\n","Technically, when `y` is not a scalar,\n","the most natural interpretation of the differentiation of a vector `y`\n","with respect to a vector `x` is a matrix.\n","For higher-order and higher-dimensional `y` and `x`,\n","the differentiation result could be a high-order tensor.\n","\n","However, while these more exotic objects do show up\n","in advanced machine learning (including in deep learning),\n","more often when we are calling backward on a vector,\n","we are trying to calculate the derivatives of the loss functions\n","for each constituent of a *batch* of training examples.\n","Here, our intent is not to calculate the differentiation matrix\n","but rather the sum of the partial derivatives\n","computed individually for each example in the batch.\n"]},{"cell_type":"code","metadata":{"origin_pos":31,"tab":["tensorflow"],"id":"0Edf82ovAFv3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596025097271,"user_tz":420,"elapsed":380,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"455533ba-9fb1-4bfa-a1e5-10d57a33efe9"},"source":["with tf.GradientTape() as t:\n","    y = x * x\n","t.gradient(y, x)  #partial derivative of y is 2*x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"origin_pos":32,"id":"7mJZL93AAFv6","colab_type":"text"},"source":["## Detaching Computation\n","\n","Sometimes, we wish to move some calculations\n","outside of the recorded computational graph.\n","For example, say that `y` was calculated as a function of `x`,\n","and that subsequently `z` was calculated as a function of both `y` and `x`.\n","Now, imagine that we wanted to calculate\n","the gradient of `z` with respect to `x`,\n","but wanted for some reason to treat `y` as a constant,\n","and only take into account the role\n","that `x` played after `y` was calculated.\n","\n","Here, we can detach `y` to return a new variable `u`\n","that has the same value as `y` but discards any information\n","about how `y` was computed in the computational graph.\n","In other words, the gradient will not flow backwards through `u` to `x`.\n","Thus, the following backpropagation function computes\n","the partial derivative of `z = u * x` with respect to `x` while treating `u` as a constant,\n","instead of the partial derivative of `z = x * x * x` with respect to `x`.\n"]},{"cell_type":"code","metadata":{"origin_pos":35,"tab":["tensorflow"],"id":"QVvZPTJ5AFv6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596025827277,"user_tz":420,"elapsed":445,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"b8c9735b-c6d0-421a-8488-cc5e9a16fc33"},"source":["# Set `persistent=True` to run `t.gradient` more than once\n","with tf.GradientTape(persistent=True) as t:\n","    y = x * x\n","    u = tf.stop_gradient(y)\n","    z = u * x\n","\n","x_grad = t.gradient(z, x)\n","x_grad == u"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"CVObh1ADj7CK","colab_type":"text"},"source":["The gradient of `z` if `y` was not detached, would have been a derivative of `x**3` which is `3x**2`."]},{"cell_type":"code","metadata":{"id":"zrD6O7qHkdKJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596026144200,"user_tz":420,"elapsed":404,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"af7895eb-4636-4c97-8c44-eb72211b0099"},"source":["print(3*(x**2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([ 0.  3. 12. 27.], shape=(4,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P5QzkGXvjcps","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596025874633,"user_tz":420,"elapsed":423,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"9fd55ff4-1380-49cb-9bf3-8b45b184987f"},"source":["#practice\n","print(x_grad)\n","print(u)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([0. 1. 4. 9.], shape=(4,), dtype=float32)\n","tf.Tensor([0. 1. 4. 9.], shape=(4,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":36,"id":"XWUNB2amAFv9","colab_type":"text"},"source":["Since the computation of `y` was recorded,\n","we can subsequently invoke backpropagation on `y` to get the derivative of `y = x * x` with respect to `x`, which is `2 * x`.\n"]},{"cell_type":"code","metadata":{"origin_pos":39,"tab":["tensorflow"],"id":"LNZVfksuAFv9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596025969841,"user_tz":420,"elapsed":379,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"e6a8ebe0-6c1a-4826-e1ee-206097eb8870"},"source":["t.gradient(y, x) == 2 * x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"origin_pos":40,"id":"2H7gL4nNAFwB","colab_type":"text"},"source":["## Computing the Gradient of Python Control Flow\n","\n","One benefit of using automatic differentiation\n","is that even if building the computational graph of a function\n","required passing through a maze of Python control flow\n","(e.g., conditionals, loops, and arbitrary function calls),\n","we can still calculate the gradient of the resulting variable.\n","In the following snippet, note that\n","the number of iterations of the `while` loop\n","and the evaluation of the `if` statement\n","both depend on the value of the input `a`.\n"]},{"cell_type":"code","metadata":{"origin_pos":43,"tab":["tensorflow"],"id":"UpzVyoD7AFwC","colab_type":"code","colab":{}},"source":["def f(a):\n","    b = a * 2\n","    #tf.norm refers to magnitude/length of the vector\n","    #keep increasing the vector b until its length exceeds 1000\n","    while tf.norm(b) < 1000:\n","        b = b * 2\n","    #if total sum of the components/elements in b > 0\n","    if tf.reduce_sum(b) > 0:\n","        c = b\n","    else:\n","        c = 100 * b\n","    return c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":44,"id":"YR3KUAIzAFwF","colab_type":"text"},"source":["Let us compute the gradient.\n"]},{"cell_type":"code","metadata":{"origin_pos":47,"tab":["tensorflow"],"id":"cDCM6vRiAFwF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596027352751,"user_tz":420,"elapsed":409,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"0b52be42-c57e-47f0-e5bb-50b93f0e8690"},"source":["a = tf.Variable(tf.random.normal(shape=()))\n","print(f\"a = {a}\")\n","with tf.GradientTape() as t:\n","    #d stores the return value which is c in function f(a)\n","    d = f(a)\n","    print(f\"d = {d}\")\n","d_grad = t.gradient(d, a)\n","d_grad"],"execution_count":null,"outputs":[{"output_type":"stream","text":["a = <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.6778144>\n","d = 1388.1639404296875\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=2048.0>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"origin_pos":48,"id":"tkEgRqToAFwH","colab_type":"text"},"source":["We can now analyze the `f` function defined above.\n","Note that it is piecewise linear in its input `a`.\n","In other words, for any `a` there exists some constant scalar `k`\n","such that `f(a) = k * a`, where the value of `k` depends on the input `a`.\n","Consequently `d / a` allows us to verify that the gradient is correct.\n"]},{"cell_type":"code","metadata":{"origin_pos":51,"tab":["tensorflow"],"id":"w85qHopgAFwI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596027361558,"user_tz":420,"elapsed":509,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"bd6215c3-2ec3-4a55-e85a-4a8166c98d58"},"source":["d_grad == d / a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=True>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"kBU8EIIbo9TR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596027362757,"user_tz":420,"elapsed":390,"user":{"displayName":"Dinakar Geddapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcRIoVExYcdvhSOVBJo3iiy9SWT0WI6bFoIcBHqA=s64","userId":"05110836538322591722"}},"outputId":"90d0bcf2-eab7-4d17-dc57-ac1a0df13fe2"},"source":["print(d/a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(2048.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":52,"id":"F8EC1OgqAFwK","colab_type":"text"},"source":["## Summary\n","\n","* Deep learning frameworks can automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivatives. We then record the computation of our target value, execute its function for backpropagation, and access the resulting gradient.\n","\n","\n","## Exercises\n","\n","1. Why is the second derivative much more expensive to compute than the first derivative?\n","1. After running the function for backpropagation, immediately run it again and see what happens.\n","1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or matrix. At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n","1. Redesign an example of finding the gradient of the control flow. Run and analyze the result.\n","1. Let $f(x) = \\sin(x)$. Plot $f(x)$ and $\\frac{df(x)}{dx}$, where the latter is computed without exploiting that $f'(x) = \\cos(x)$.\n"]},{"cell_type":"markdown","metadata":{"origin_pos":55,"tab":["tensorflow"],"id":"qn5ZZDr0AFwK","colab_type":"text"},"source":["[Discussions](https://discuss.d2l.ai/t/200)\n"]}]}